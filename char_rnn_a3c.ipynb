{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Spanish.txt', 'data/names/Russian.txt', 'data/names/French.txt', 'data/names/Chinese.txt', 'data/names/Czech.txt', 'data/names/Japanese.txt', 'data/names/Dutch.txt', 'data/names/Portuguese.txt', 'data/names/Arabic.txt', 'data/names/German.txt', 'data/names/Irish.txt', 'data/names/Greek.txt', 'data/names/Korean.txt', 'data/names/Polish.txt', 'data/names/Scottish.txt', 'data/names/Italian.txt', 'data/names/English.txt', 'data/names/Vietnamese.txt']\n",
      "Slusarski\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "n_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ``category_lines``, a dictionary mapping each category\n",
    "(language) to a list of lines (names). We also kept track of\n",
    "``all_categories`` (just a list of languages) and ``n_categories`` for\n",
    "later reference.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning Names into Tensors\n",
    "--------------------------\n",
    "\n",
    "Now that we have all the names organized, we need to turn them into\n",
    "Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we use a \"one-hot vector\" of size\n",
    "``<1 x n_letters>``. A one-hot vector is filled with 0s except for a 1\n",
    "at index of the current letter, e.g. ``\"b\" = <0 1 0 0 0 ...>``.\n",
    "\n",
    "To make a word we join a bunch of those into a 2D matrix\n",
    "``<line_length x 1 x n_letters>``.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in\n",
    "batches - we're just using a batch size of 1 here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Turn a category into a <1 x n_categories> one-hot vec\n",
    "def categoryToTensor(category):\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][all_categories.index(category)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(categoryToTensor('Korean').size())\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Network",
    "Based on model by Ilya Kostrikov (https://github.com/ikostrikov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_columns_initializer(weights, std=1.0):\n",
    "    out = torch.randn(weights.size())\n",
    "    out *= std / torch.sqrt(out.pow(2).sum(1, keepdim=True))\n",
    "    return out\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = np.prod(weight_shape[1:4])\n",
    "        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = weight_shape[1]\n",
    "        fan_out = weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class ActorCritic(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, condition_size):\n",
    "        \"\"\"\n",
    "        Asynchronous advantage actor critic accepts previous state and condition variable, \n",
    "        and outputs an action in vocab space\n",
    "        \"\"\"\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.inp2lat = nn.Linear(vocab_size, 64)\n",
    "        self.cond2lat = nn.Linear(condition_size, 64)\n",
    "        self.lat2lat = nn.Linear(128, 128)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(128,64)\n",
    "\n",
    "        self.critic_linear = nn.Linear(128, 1)\n",
    "        self.actor_linear = nn.Linear(128, vocab_size)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "        self.actor_linear.weight.data = normalized_columns_initializer(\n",
    "            self.actor_linear.weight.data, 0.01)\n",
    "        self.actor_linear.bias.data.fill_(0)\n",
    "        self.critic_linear.weight.data = normalized_columns_initializer(\n",
    "            self.critic_linear.weight.data, 1.0)\n",
    "        self.critic_linear.bias.data.fill_(0)\n",
    "\n",
    "        self.lstm.bias_ih.data.fill_(0)\n",
    "        self.lstm.bias_hh.data.fill_(0)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs, conditions, (hx, cx) = inputs\n",
    "        inp_lat = F.elu(self.inp2lat(inputs))\n",
    "        cond_lat = F.elu(self.cond2lat(conditions))\n",
    "        lat = F.elu(self.lat2lat(torch.cat((inputs, cond_lat), )))\n",
    "\n",
    "        hx, cx = self.lstm(lat, (hx, cx))\n",
    "        x = hx\n",
    "\n",
    "        return self.critic_linear(x), self.actor_linear(x), (hx, cx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the\n",
    "Tensor for the current letter) and a previous hidden state (which we\n",
    "initialize as zeros at first). We'll get back the output (probability of\n",
    "each language) and a next hidden state (which we keep for the next\n",
    "step).\n",
    "\n",
    "Remember that PyTorch modules operate on Variables rather than straight\n",
    "up Tensors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========\n",
    "Preparing for Training\n",
    "----------------------\n",
    "\n",
    "Before going into training we should make a few helper functions. The\n",
    "first is to interpret the output of the network, which we know to be a\n",
    "likelihood of each category. We can use ``Tensor.topk`` to get the index\n",
    "of the greatest value:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Italian', 15)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want a quick way to get a training example (a name and its\n",
    "language):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = Italian / line = Bologna / category_tensor = torch.Size([1, 18]) / line_tensor = torch.Size([7, 1, 57])\n",
      "category = Scottish / line = Cameron / category_tensor = torch.Size([1, 18]) / line_tensor = torch.Size([7, 1, 57])\n",
      "category = Greek / line = Chellos / category_tensor = torch.Size([1, 18]) / line_tensor = torch.Size([7, 1, 57])\n",
      "category = Korean / line = Son / category_tensor = torch.Size([1, 18]) / line_tensor = torch.Size([3, 1, 57])\n",
      "category = Dutch / line = Koolen / category_tensor = torch.Size([1, 18]) / line_tensor = torch.Size([6, 1, 57])\n",
      "category = Italian / line = Caivano / category_tensor = torch.Size([1, 18]) / line_tensor = torch.Size([7, 1, 57])\n",
      "category = Japanese / line = Shunsen / category_tensor = torch.Size([1, 18]) / line_tensor = torch.Size([7, 1, 57])\n",
      "category = Czech / line = Hajek / category_tensor = torch.Size([1, 18]) / line_tensor = torch.Size([5, 1, 57])\n",
      "category = Arabic / line = Sarkis / category_tensor = torch.Size([1, 18]) / line_tensor = torch.Size([6, 1, 57])\n",
      "category = Spanish / line = Zubizarreta / category_tensor = torch.Size([1, 18]) / line_tensor = torch.Size([11, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = Variable(categoryToTensor(category))\n",
    "    line_tensor = Variable(lineToTensor(line))\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line,\n",
    "          '/ category_tensor =', category_tensor.shape,\n",
    "          '/ line_tensor =', line_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Network\n",
    "--------------------\n",
    "\n",
    "Now all it takes to train this network is show it a bunch of examples,\n",
    "have it make guesses, and tell it if it's wrong.\n",
    "\n",
    "For the loss function ``nn.NLLLoss`` is appropriate, since the last\n",
    "layer of the RNN is ``nn.LogSoftmax``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "max_name_length = 10\n",
    "max_episode_length = 10000\n",
    "gamma = 0.99\n",
    "tau = 1.00\n",
    "entropy_coef = 0.01\n",
    "value_loss_coef = 0.5\n",
    "max_grad_norm = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import create_atari_env\n",
    "\n",
    "def train(model, optimizer=None):\n",
    "\n",
    "    env = create_atari_env(args.env_name)\n",
    "    env.seed(args.seed + rank)\n",
    "\n",
    "    model = ActorCritic(env.observation_space.shape[0], env.action_space)\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state)\n",
    "    done = True\n",
    "\n",
    "    episode_length = 0\n",
    "    while True:\n",
    "        if done:\n",
    "            cx = Variable(torch.zeros(1, 256))\n",
    "            hx = Variable(torch.zeros(1, 256))\n",
    "        else:\n",
    "            cx = Variable(cx.data)\n",
    "            hx = Variable(hx.data)\n",
    "\n",
    "        values = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        entropies = []\n",
    "\n",
    "        for step in range(max_name_length):\n",
    "            episode_length += 1\n",
    "            value, logit, (hx, cx) = model((Variable(state.unsqueeze(0)),\n",
    "                                            Variable(,\n",
    "                                            (hx, cx)))\n",
    "            prob = F.softmax(logit)\n",
    "            log_prob = F.log_softmax(logit)\n",
    "            entropy = -(log_prob * prob).sum(1, keepdim=True)\n",
    "            entropies.append(entropy)\n",
    "\n",
    "            action = prob.multinomial().data\n",
    "            log_prob = log_prob.gather(1, Variable(action))\n",
    "\n",
    "            state, reward, done, _ = env.step(action.numpy())\n",
    "            done = done or episode_length >= max_episode_length\n",
    "            reward = max(min(reward, 1), -1)\n",
    "\n",
    "            with lock:\n",
    "                counter.value += 1\n",
    "\n",
    "            if done:\n",
    "                episode_length = 0\n",
    "                state = env.reset()\n",
    "\n",
    "            state = torch.from_numpy(state)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        R = torch.zeros(1, 1)\n",
    "        if not done:\n",
    "            value, _, _ = model((Variable(state.unsqueeze(0)), (hx, cx)))\n",
    "            R = value.data\n",
    "\n",
    "        values.append(Variable(R))\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        R = Variable(R)\n",
    "        gae = torch.zeros(1, 1)\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = gamma * R + rewards[i]\n",
    "            advantage = R - values[i]\n",
    "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
    "\n",
    "            # Generalized Advantage Estimataion\n",
    "            delta_t = rewards[i] + gamma * \\\n",
    "                values[i + 1].data - values[i].data\n",
    "            gae = gae * gamma * tau + delta_t\n",
    "\n",
    "            policy_loss = policy_loss - \\\n",
    "                log_probs[i] * Variable(gae) - entropy_coef * entropies[i]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        (policy_loss + value_loss_coef * value_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), max_grad_norm)\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 57 distinct ascii characters, 18 languages\n",
    "model = ActorCritic(n_letters, n_categories)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
